{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed987d9b-e3e7-4717-9257-b3901c52f844",
   "metadata": {
    "name": "_Intro",
    "collapsed": false
   },
   "source": "# âš½ **HOL: Eredivisie Prediction** ðŸ¥‡\n### Notebook - Model Training - 3/4\n\n---\n\n\n### What We'll Do:\n1. **Data Ingestion**: Fetch Eredivisie data from the GitHub repository.\n2. **Data Transformation**: Utilize Snowpark DataFrames for data preparation and analysis.\n3. -> **Model Training**: Train model and store it in the Snowflake Model Registry\n4. **Prediction**: Predict who is going to win Eredivisie 2024/2025\n\n![image](https://i.makeagif.com/media/2-26-2017/iTVOpv.gif)\n"
  },
  {
   "cell_type": "markdown",
   "id": "0f2f221c-19c2-45b7-9a9b-2f593ffc490f",
   "metadata": {
    "name": "_Step_3_Model_Training",
    "collapsed": false
   },
   "source": "\n## Step 3: Model Training and Evaluation\n---\n\nIn this notebook we'll perform the following activities:\n\n- Hyperparameter Tuning\n- Model Training\n- Model Validation\n- Model Registry\n\n### Setup\n\nBefore using this notebook, ensure that you have imported the following packages by click on the top right \"Packages\" button and restart the notebook:\n\n- `snowflake-snowpark-python` (Latest)\n- `snowflake-ml-python` (Latest)\n- `fastparquet` (Latest)\n"
  },
  {
   "cell_type": "code",
   "id": "5e670b31-0dba-4d9a-89cd-d58bf542679e",
   "metadata": {
    "language": "python",
    "name": "Import_Libs",
    "collapsed": false
   },
   "outputs": [],
   "source": "import snowflake.snowpark\n#import pandas as pd\n#import numpy as np\nimport streamlit as st\nfrom snowflake.snowpark.session import Session\nfrom snowflake.snowpark import Window\nfrom snowflake.snowpark import functions as F   \nfrom snowflake.snowpark.functions import udf, udtf\nfrom snowflake.snowpark.types import IntegerType, FloatType, StringType, StructField, StructType, DateType\n\nimport warnings\nwarnings.filterwarnings('ignore')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98a6d186-f20e-4939-af53-651379fa73cd",
   "metadata": {
    "language": "python",
    "name": "Get_Active_Session",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10f33abb-d7b1-4c0f-b45a-dc5fbb45dbda",
   "metadata": {
    "language": "python",
    "name": "Function_MLOPS_Versioning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# FUNCTION used to iterate the model version so we can automatically \n# create the next version number\n\nimport ast\n\ndef get_next_version(reg, model_name) -> str:\n    \"\"\"\n    Returns the next version of a model based on the existing versions in the registry.\n\n    Args:\n        reg: The registry object that provides access to the models.\n        model_name: The name of the model.\n\n    Returns:\n        str: The next version of the model in the format \"V_<version_number>\".\n\n    Raises:\n        ValueError: If the version list for the model is empty or if the version format is invalid.\n    \"\"\"\n    models = reg.show_models()\n    if models.empty:\n        return \"V_1\"\n    elif model_name not in models[\"name\"].to_list():\n        return \"V_1\"\n    max_version_number = max(\n        [\n            int(version.split(\"_\")[-1])\n            for version in ast.literal_eval(\n                models.loc[models[\"name\"] == model_name, \"versions\"].values[0]\n            )\n        ]\n    )\n    return f\"V_{max_version_number + 1}\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40b6b856-4545-40d9-b99d-0b6a93858329",
   "metadata": {
    "language": "python",
    "name": "Check_Distribution",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\nfrom snowflake.snowpark.functions import col\nfrom snowflake.snowpark import Session\nfrom snowflake.ml.modeling.preprocessing import LabelEncoder\nimport snowflake.snowpark.functions as F\n\n# Check distribution to see how balanced out data set is\ndf_training = session.table(f'eredivisie_features')\n\n# Load df_training from Snowflake table\ndf_training = session.table('eredivisie_features').filter(\n    col(\"DATE\") > '2010-08-01')\n\n# Filter rows where GAME_OUTCOME = 0 (draw)\ndf_training_draw_matches = df_training.filter(col('GAME_OUTCOME') == 1)\n\n# Randomly select 2000 rows from outcome_2_df\nrows_to_drop = df_training_draw_matches.sample(n=500)  \n\n# Drop the selected rows from the original DataFrame\ndf_training_balanced = df_training.join(\n    rows_to_drop,\n    on=list(rows_to_drop.columns),\n    how='left_anti'  # This will keep only the rows not in rows_to_drop\n)\n\ndf_training_balanced.group_by('GAME_OUTCOME').agg(F.count('*')).sort(F.col('GAME_OUTCOME'))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3714061e-55c8-4fa1-aadf-447f6372a25c",
   "metadata": {
    "language": "python",
    "name": "Check_Dataframe",
    "collapsed": false
   },
   "outputs": [],
   "source": "st.dataframe(df_training_balanced.limit(20))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d9f54025-3b5b-4931-b37d-83467dce796c",
   "metadata": {
    "language": "sql",
    "name": "Scale_Up_Warehouse",
    "collapsed": false
   },
   "outputs": [],
   "source": "--If we want to run some hyperparameter tuning, in order to speed things up lets size up -our warehouse. This scale up is just temporary, we'll scale it down for training.\n--Be Aware of possible costs associated running this warehouse size.\n--Wait that the WH is actually scaled up before running the next cell.\n\nalter warehouse EREDIVISIE_PREDICTION_WH set warehouse_size = xxlarge",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3543b6b-dfce-4ca6-ad05-cfe0e59eaa0c",
   "metadata": {
    "language": "python",
    "name": "Hyperparameter_Tuning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# To Uncomment this cell -> Select All -> Cmd + / (on Mac)\n# Hyper Parameter tuning will allow to get the best parameters to train our model\n# This step should take about 4 mins, on a 2XL.  \n# Be Aware of possible costs associated executing this cell.\n# If you scale up the warehouse, you'll scale it down in the next step.\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection.grid_search_cv import GridSearchCV\n\nFEATURE_COLS = [\"HOME_WINS_LAST68\",\"HOME_WIN_PERCENTAGE_LAST68\", \"H2H_HOME_WINS_LAST10\" , \"H2H_HOME_LOSSES_LAST10\", \"HOME_GOALS_AGAINST_LAST34\"]\nLABEL_COLS = [\"GAME_OUTCOME\"]\n\n# Select only the required feature and label columns for training\ndf_training_filtered = df_training_balanced.select(FEATURE_COLS + LABEL_COLS)\n\nhyperparam_grid = {\n    \"n_estimators\": [50, 100, 200],\n    \"learning_rate\": [0.05, 0.1, 0.2],\n    \"max_depth\": [3, 4, 5]\n}\n\npipeline = Pipeline(\n    steps = [\n        (\n            \"scaler\", \n            StandardScaler(\n                input_cols=FEATURE_COLS, \n                output_cols=FEATURE_COLS\n            )\n        ),\n        (\n        \"GridSearchCV\",\n            GridSearchCV(\n                estimator=XGBClassifier(random_state=42),\n                param_grid=hyperparam_grid,\n                scoring='accuracy', \n                label_cols=LABEL_COLS,\n                input_cols=FEATURE_COLS\n            )   \n        )\n    ]\n)\n\npipeline.fit(df_training_filtered)\n\nsklearn_hp = pipeline.to_sklearn()\noptimal_params = sklearn_hp.steps[-1][1].best_params_\nscore_dict = {\"best_accuracy\": sklearn_hp.steps[-1][1].best_score_}\n\nst.write(score_dict)\nst.write(optimal_params)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98ef2e7a-4688-426f-a3b1-5bcef44417d3",
   "metadata": {
    "language": "sql",
    "name": "Scale_Down_Warehouse",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- now we can scale it back down, in a matter of seconds\n\nalter warehouse eredivisie_prediction_wh set warehouse_size = xsmall",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "87bb3d6a-3d50-41f6-a7ea-3733660e89dc",
   "metadata": {
    "language": "python",
    "name": "Model_Training",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# taking our optimal parameters we're going to build our model\n\nfrom snowflake.ml.modeling.preprocessing import StandardScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.metrics import *\n\n\nFEATURE_COLS = [\"HOME_WINS_LAST34\",\"HOME_WIN_PERCENTAGE_LAST34\", \"H2H_HOME_WINS_LAST10\" , \"H2H_HOME_LOSSES_LAST10\", \"HOME_GOALS_AGAINST_LAST34\"]\nLABEL_COLS = [\"GAME_OUTCOME\"]\n\n# Select only the required feature and label columns for training\ndf_training_filtered = df_training_balanced.select(FEATURE_COLS + LABEL_COLS)\n\n# Split the filtered dataframe into training and test datasets\ntrain_data, test_data = df_training_filtered.random_split(weights=[0.8, 0.2], seed=0)\n\n# Optimal params: max_depth= 3, n_estimators = 100, learning_rate = 0.3\npipeline = Pipeline(\n    steps = [\n        (\n            \"scaler\", \n            StandardScaler(\n                input_cols=FEATURE_COLS, \n                output_cols=FEATURE_COLS\n            )\n        ),\n        (\n            \"model\", \n            XGBClassifier(\n                input_cols=FEATURE_COLS, \n                label_cols=LABEL_COLS,\n                max_depth= optimal_params['max_depth'],\n                n_estimators = optimal_params['n_estimators'],\n                learning_rate = optimal_params['learning_rate']\n            )\n        )\n    ]\n)\n\npipeline.fit(train_data)\n\n# Get the model accuracy\npredict_on_training_data = pipeline.predict(train_data)\ntraining_accuracy = accuracy_score(df=predict_on_training_data, y_true_col_names=[\"GAME_OUTCOME\"], y_pred_col_names=[\"OUTPUT_GAME_OUTCOME\"])\n\npredict_on_test_data = pipeline.predict(test_data)\neval_accuracy = accuracy_score(df=predict_on_test_data, y_true_col_names=[\"GAME_OUTCOME\"], y_pred_col_names=[\"OUTPUT_GAME_OUTCOME\"])\n\nst.write(\"Model Training Completed\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2e28f0b-f349-4673-9087-f2d781ea36ad",
   "metadata": {
    "language": "python",
    "name": "Model_Validation",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Homework - You can plot some additional statistics!\nst.write(f\"Training accuracy: {training_accuracy} \\nEval accuracy: {eval_accuracy}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac1dd149-42d1-48e5-a125-d312ff172787",
   "metadata": {
    "name": "_Model_Registry",
    "collapsed": false
   },
   "source": "## Model Registry\n---\n\n- Once the model is ready we'll use it to predict results of group stage.\n- Save the model using MLOps Model Registry features."
  },
  {
   "cell_type": "code",
   "id": "0c6956be-1db9-42e8-80cd-adc880c86783",
   "metadata": {
    "language": "python",
    "name": "Model_Registration",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\nreg = Registry(session=session)\n\nmodel_name = \"EREDIVISIE_PREDICT\"\nmodel_version = get_next_version(reg, model_name)\n\nreg.log_model(\n    model_name=model_name,\n    version_name=model_version,\n    model=pipeline,\n    metrics={'training_accuracy':training_accuracy, 'eval_accuracy':eval_accuracy},\n    options={'relax_version': False}\n)\n\nm = reg.get_model(model_name)\nm.default = model_version",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f77d1126-ee25-452e-a0e6-fc4f57750b53",
   "metadata": {
    "language": "python",
    "name": "Model_Versioning",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# lets see the models we have in our registry\n\nreg.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f57f506-5097-4129-984d-d78f806473b1",
   "metadata": {
    "name": "_Summary",
    "collapsed": false
   },
   "source": "# Summary\n\nWe now have a model in our registry we can use to call from either Snowpark or SQL, which we'll use in the predictions notebook"
  }
 ]
}