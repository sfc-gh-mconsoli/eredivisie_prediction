{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b402a9c-84ec-4818-ab34-967764d3f6fc",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# âš½ **HOL: Eredivisie Prediction** ðŸ¥‡\n\n---\n## Step 1: Data Ingestion\n\nWelcome to the **Eredivisie Prediction**! In this notebook, we will use Python functions and External Access Integration to load and analyze data about Eredivisie from 1995 to 2023. Our data source is a GitHub repository, from which we'll fetch and directly store historical data in our Snowflake account. No S3 buckets or local downloads are needed â€” our goal is to simplify the execution of this Hands-On Lab (HOL) while showcasing the extensive capabilities of Snowflake!\n\n### What We'll Do:\n1. **Load Data**: Fetch Olympic data from the GitHub repository.\n2. **Analyze Data**: Utilize Snowpark DataFrames for data preparation and analysis.\n3. **Visualize Insights**: Build interactive dashboards with Streamlit for comprehensive analytics.\n\n![Olympic Rings](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Eredivisie_nieuw_logo_2017-.svg/640px-Eredivisie_nieuw_logo_2017-.svg.png)\n\n---\n\nDive into the code below to start loading data!\n"
  },
  {
   "cell_type": "markdown",
   "id": "898f2e7c-3f14-41a1-baeb-c1a48579f43e",
   "metadata": {
    "name": "Pre_Reqs",
    "collapsed": false
   },
   "source": "### Setup\n\nBefore using this notebook, ensure that you have created the following objects by running the `setup.sql` script in a worksheet:\n\n- **Database**: `EREDIVISIE_PREDICTION`\n- **Schema**: `RAW_DATA`\n- **Warehouse**: `EREDIVISIE_PREDICTION_WH`\n- **Network Rule**: `GITHUB_NETWORK_RULE`\n- **External Access Integration**: `GITHUB_EXTERNAL_ACCESS_INTEGRATION`\n\nThe first three items are required as you will need to define the Database, Schema, and Warehouse when you import this notebook into the Snowflake UI.\n\nFor the **Network Rule** and **External Access Integration**, once created, follow these steps to make them available within this notebook:\n\n1. **Click on Notebook Settings** (located at the top right of the worksheet screen).\n2. **Select the External Access Tab**.\n3. **Enable** `GITHUB_EXTERNAL_ACCESS_INTEGRATION` from the list.\n4. **Reload the Notebook**. Once reloaded, you will have access to the GitHub URL directly from this notebook.\n\nWith these configurations in place, youâ€™ll be ready to extract and work with the dataset from the external GitHub URL in the following cells.\n\n---\n"
  },
  {
   "cell_type": "code",
   "id": "9908f3ab-97f1-4610-8674-8655b80862d3",
   "metadata": {
    "language": "python",
    "name": "Get_Active_Session",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import Session\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e00475bc-49df-4635-bd32-f4c8eb4c8225",
   "metadata": {
    "language": "sql",
    "name": "Use_Role_Enforcement",
    "collapsed": false
   },
   "outputs": [],
   "source": "--Note: For this Hands-On Lab (HOL), we are not creating ad hoc roles and users to minimize prerequisites and simplify setup.\n\nUSE ROLE ACCOUNTADMIN;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "208cbcf7-db40-4a95-8f53-518bb1fe92a1",
   "metadata": {
    "language": "python",
    "name": "Create_Fetch_Data_Function",
    "collapsed": false
   },
   "outputs": [],
   "source": "import requests\nimport pandas as pd\nfrom snowflake.snowpark import DataFrame as df\nfrom io import StringIO\n\ndef fetch_dataset_from_github(url: str) -> 'DataFrame':\n    # Fetch the CSV data from the URL\n    response = requests.get(url)\n    if response.status_code == 200:\n        # Decode the content and read into a Pandas DataFrame\n        csv_data = response.content.decode('utf-8')\n        csv_file = StringIO(csv_data)\n        pandas_df = pd.read_csv(csv_file)\n        \n        # Convert Pandas DataFrame to Snowpark DataFrame\n        return session.create_dataframe(pandas_df)\n    else:\n        raise Exception(f\"Failed to fetch CSV: {response.status_code} - {response.text}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00876816-7aa0-4300-8373-5de754927d78",
   "metadata": {
    "language": "python",
    "name": "Load_All_Data",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Once an updated file is on the github dataset repo, it's enoguh to run this cell to reload the new datasets.\nfrom snowflake.snowpark.functions import col\n\n# Function to rename columns to uppercase\ndef rename_columns_to_uppercase(df):\n    # Generate a list of columns with uppercase names\n    new_columns = [col(c).alias(c.upper()) for c in df.columns]\n    # Select columns with new names\n    return df.select(*new_columns)\n\n# Base URL and list of files\nurl_base = 'https://github.com/sfc-gh-mconsoli/eredivisie_prediction/raw/main/dataset/'\nurl_files = [\n    'eredivisie_history.csv',\n    'eredivisie_fixture.csv'\n]\n\n# Loop through each URL\nfor url in url_files:\n    # Get Snowpark DataFrame from the URL\n    df = fetch_dataset_from_github(url_base + url)\n\n    # Extract table name from URL\n    table_name = url.split('/')[-1].replace('.csv', '').upper()\n\n    # Drop the table if it exists\n    session.sql(f\"DROP TABLE IF EXISTS {table_name}\").collect()\n\n    # Convert column names to uppercase\n    df = rename_columns_to_uppercase(df)\n\n    # Create table and insert data from Snowpark DataFrame\n    df.write.save_as_table(table_name, mode='overwrite')\n\n    print(f\"Table {table_name} created and data loaded successfully.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fc221afd-5cf4-4877-81f2-341b5a794274",
   "metadata": {
    "language": "python",
    "name": "Verify_Data_Loaded_1",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's check loaded data\n\nsession.table('EREDIVISIE_HISTORY').limit(51)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "db058f60-3e5b-4a2f-863d-5aa0ff228e59",
   "metadata": {
    "language": "sql",
    "name": "Verify_Data_Loaded_2",
    "collapsed": false
   },
   "outputs": [],
   "source": "--NOTE: It seems we might need to transform some data or maybe there are some issue in the CSV.\nSELECT YEAR(TO_DATE(\"DATE\",'DD/MM/YYYY')), COUNT(*)\nFROM EREDIVISIE_PREDICTION.PUBLIC.EREDIVISIE_HISTORY\nGROUP BY YEAR(TO_DATE(\"DATE\",'DD/MM/YYYY'))\norder by YEAR(TO_DATE(\"DATE\",'DD/MM/YYYY'))  DESC",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d80c942b-3f67-4594-b779-4ec271a57374",
   "metadata": {
    "language": "python",
    "name": "Winner_Analysis",
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\nimport snowflake.snowpark as sp\n\n#Let's get number of match won by each Team in the last XYZ years\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9be3585a-985e-484a-b330-dea12b9f9ad5",
   "metadata": {
    "language": "python",
    "name": "Streamlit_in_Notebooks",
    "collapsed": false
   },
   "outputs": [],
   "source": "import plotly.graph_objects as go\n\n# Draw a chart with statistics extracted above.",
   "execution_count": null
  }
 ]
}